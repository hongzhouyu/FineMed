{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "q2text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"/qwen72b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Dataset({\n",
       "     features: ['question'],\n",
       "     num_rows: 209971\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['question'],\n",
       "     num_rows: 209971\n",
       " }))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "question1_set = load_from_disk(\"data/instructions/gen_ins/ins1\")\n",
    "question2_set = load_from_disk(\"data/instructions/gen_ins/ins2\")\n",
    "question1_set, question2_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'question': 'What factors contribute to the low efficiency of cationic lipid-mediated gene transduction in polarized and differentiated airway epithelial cells, and how might these barriers be overcome to improve gene therapy for airway diseases such as cystic fibrosis?'},\n",
       " {'question': 'How do the results of gene transduction using GL-67:pDNA complexes in the nasal epithelium of cystic fibrosis transgenic mice compare to those observed in the lungs, and what implications do these differences have for the development of gene therapy strategies for cystic fibrosis?'})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question1_set[0], question2_set[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nYou need to evaluate the given query based on the following criteria and output the results in JSON format. The output should include three parts: quality, difficulty, and whether additional necessary information is required to answer the query. Please follow the scoring standards below:\\n1. Quality (Score 1-10): Assess the clarity and accuracy of the query. If the query is a simple statement without any question or instruction, score it 1-2.\\n    9-10: Very clear, accurate expression, no ambiguity.\\n    7-8: Clear, accurate expression, but may have minimal ambiguity.\\n    5-6: Fairly clear, generally accurate expression, but some ambiguity exists.\\n    3-4: Not very clear, somewhat vague expression, with obvious ambiguity.\\n    1-2: Unclear, very vague expression, difficult to understand or a simple statement.\\n2. Difficulty (Score 1-10): Assess the difficulty of understanding and answering the query.\\n    9-10: Very difficult, requires specialized knowledge and complex analysis to answer.\\n    7-8: Quite difficult, requires some specialized knowledge and analysis.\\n    5-6: Moderate difficulty, requires general knowledge and analysis.\\n    3-4: Fairly simple, can be answered with basic knowledge.\\n    1-2: Very simple, no special knowledge required to answer.\\n3. Relevance to medicine (1-6): Assess the medical relevance of the query.\\n    5-6: Completely related to medicine, with many medical terms appearing.\\n    3-4: Related to medicine, the content involves medical fields.\\n    1-2: Very weak medical relevance.\\n4. Mention specific details: Whether specific case details in the text are mentioned.\\nPlease only output the results without any explanation and strictly follow the format below for output:\\n{\\n  \"quality\": 1-10,\\n  \"difficulty\": 1-10,\\n  \"relevance_to_medicine\": 1-6,\\n  \"mention_specific_details\": \"True\"/\"False\"\\n}\\nHere is the query:\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = '''\n",
    "You need to evaluate the given query based on the following criteria and output the results in JSON format. The output should include three parts: quality, difficulty, and whether additional necessary information is required to answer the query. Please follow the scoring standards below:\n",
    "1. Quality (Score 1-10): Assess the clarity and accuracy of the query. If the query is a simple statement without any question or instruction, score it 1-2.\n",
    "    9-10: Very clear, accurate expression, no ambiguity.\n",
    "    7-8: Clear, accurate expression, but may have minimal ambiguity.\n",
    "    5-6: Fairly clear, generally accurate expression, but some ambiguity exists.\n",
    "    3-4: Not very clear, somewhat vague expression, with obvious ambiguity.\n",
    "    1-2: Unclear, very vague expression, difficult to understand or a simple statement.\n",
    "2. Difficulty (Score 1-10): Assess the difficulty of understanding and answering the query.\n",
    "    9-10: Very difficult, requires specialized knowledge and complex analysis to answer.\n",
    "    7-8: Quite difficult, requires some specialized knowledge and analysis.\n",
    "    5-6: Moderate difficulty, requires general knowledge and analysis.\n",
    "    3-4: Fairly simple, can be answered with basic knowledge.\n",
    "    1-2: Very simple, no special knowledge required to answer.\n",
    "3. Relevance to medicine (1-6): Assess the medical relevance of the query.\n",
    "    5-6: Completely related to medicine, with many medical terms appearing.\n",
    "    3-4: Related to medicine, the content involves medical fields.\n",
    "    1-2: Very weak medical relevance.\n",
    "4. Mention specific details: Whether specific case details in the text are mentioned.\n",
    "Please only output the results without any explanation and strictly follow the format below for output:\n",
    "{\n",
    "  \"quality\": 1-10,\n",
    "  \"difficulty\": 1-10,\n",
    "  \"relevance_to_medicine\": 1-6,\n",
    "  \"mention_specific_details\": \"True\"/\"False\"\n",
    "}\n",
    "Here is the query:\n",
    "'''\n",
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def raw2text(example):\n",
    "    text_parts = ['You need to evaluate the given query based on the following criteria and output the results in JSON format. The output should include three parts: quality, difficulty, and whether additional necessary information is required to answer the query. Please follow the scoring standards below:\\n1. Quality (Score 1-10): Assess the clarity and accuracy of the query. If the query is a simple statement without any question or instruction, score it 1-2.\\n    9-10: Very clear, accurate expression, no ambiguity.\\n    7-8: Clear, accurate expression, but may have minimal ambiguity.\\n    5-6: Fairly clear, generally accurate expression, but some ambiguity exists.\\n    3-4: Not very clear, somewhat vague expression, with obvious ambiguity.\\n    1-2: Unclear, very vague expression, difficult to understand or a simple statement.\\n2. Difficulty (Score 1-10): Assess the difficulty of understanding and answering the query.\\n    9-10: Very difficult, requires specialized knowledge and complex analysis to answer.\\n    7-8: Quite difficult, requires some specialized knowledge and analysis.\\n    5-6: Moderate difficulty, requires general knowledge and analysis.\\n    3-4: Fairly simple, can be answered with basic knowledge.\\n    1-2: Very simple, no special knowledge required to answer.\\n3. Relevance to medicine (1-6): Assess the medical relevance of the query.\\n    5-6: Completely related to medicine, with many medical terms appearing.\\n    3-4: Related to medicine, the content involves medical fields.\\n    1-2: Very weak medical relevance.\\n4. Mention specific details: Whether specific case details in the text are mentioned.\\nPlease only output the results without any explanation and strictly follow the format below for output:\\n{\\n  \"quality\": 1-10,\\n  \"difficulty\": 1-10,\\n  \"relevance_to_medicine\": 1-6,\\n  \"mention_specific_details\": \"True\"/\"False\"\\n}\\nHere is the query:\\n']\n",
    "    text_parts.append(example[\"question\"])\n",
    "    \n",
    "    text = \"\".join(text_parts)\n",
    "    text += \"\\nOutput: \\n\" \n",
    "    example[\"text\"] = text\n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/209971 [00:00<?, ? examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 209971/209971 [00:06<00:00, 32019.92 examples/s]\n",
      "Map: 100%|██████████| 209971/209971 [00:06<00:00, 31582.46 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(Dataset({\n",
       "     features: ['text'],\n",
       "     num_rows: 209971\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['text'],\n",
       "     num_rows: 209971\n",
       " }))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question1_set = question1_set.map(raw2text, remove_columns=question1_set.column_names)\n",
    "question2_set = question2_set.map(raw2text, remove_columns=question2_set.column_names)\n",
    "question1_set, question2_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'text': 'You need to evaluate the given query based on the following criteria and output the results in JSON format. The output should include three parts: quality, difficulty, and whether additional necessary information is required to answer the query. Please follow the scoring standards below:\\n1. Quality (Score 1-10): Assess the clarity and accuracy of the query. If the query is a simple statement without any question or instruction, score it 1-2.\\n    9-10: Very clear, accurate expression, no ambiguity.\\n    7-8: Clear, accurate expression, but may have minimal ambiguity.\\n    5-6: Fairly clear, generally accurate expression, but some ambiguity exists.\\n    3-4: Not very clear, somewhat vague expression, with obvious ambiguity.\\n    1-2: Unclear, very vague expression, difficult to understand or a simple statement.\\n2. Difficulty (Score 1-10): Assess the difficulty of understanding and answering the query.\\n    9-10: Very difficult, requires specialized knowledge and complex analysis to answer.\\n    7-8: Quite difficult, requires some specialized knowledge and analysis.\\n    5-6: Moderate difficulty, requires general knowledge and analysis.\\n    3-4: Fairly simple, can be answered with basic knowledge.\\n    1-2: Very simple, no special knowledge required to answer.\\n3. Relevance to medicine (1-6): Assess the medical relevance of the query.\\n    5-6: Completely related to medicine, with many medical terms appearing.\\n    3-4: Related to medicine, the content involves medical fields.\\n    1-2: Very weak medical relevance.\\n4. Mention specific details: Whether specific case details in the text are mentioned.\\nPlease only output the results without any explanation and strictly follow the format below for output:\\n{\\n  \"quality\": 1-10,\\n  \"difficulty\": 1-10,\\n  \"relevance_to_medicine\": 1-6,\\n  \"mention_specific_details\": \"True\"/\"False\"\\n}\\nHere is the query:\\nWhat factors contribute to the low efficiency of cationic lipid-mediated gene transduction in polarized and differentiated airway epithelial cells, and how might these barriers be overcome to improve gene therapy for airway diseases such as cystic fibrosis?\\nOutput: \\n'},\n",
       " {'text': 'You need to evaluate the given query based on the following criteria and output the results in JSON format. The output should include three parts: quality, difficulty, and whether additional necessary information is required to answer the query. Please follow the scoring standards below:\\n1. Quality (Score 1-10): Assess the clarity and accuracy of the query. If the query is a simple statement without any question or instruction, score it 1-2.\\n    9-10: Very clear, accurate expression, no ambiguity.\\n    7-8: Clear, accurate expression, but may have minimal ambiguity.\\n    5-6: Fairly clear, generally accurate expression, but some ambiguity exists.\\n    3-4: Not very clear, somewhat vague expression, with obvious ambiguity.\\n    1-2: Unclear, very vague expression, difficult to understand or a simple statement.\\n2. Difficulty (Score 1-10): Assess the difficulty of understanding and answering the query.\\n    9-10: Very difficult, requires specialized knowledge and complex analysis to answer.\\n    7-8: Quite difficult, requires some specialized knowledge and analysis.\\n    5-6: Moderate difficulty, requires general knowledge and analysis.\\n    3-4: Fairly simple, can be answered with basic knowledge.\\n    1-2: Very simple, no special knowledge required to answer.\\n3. Relevance to medicine (1-6): Assess the medical relevance of the query.\\n    5-6: Completely related to medicine, with many medical terms appearing.\\n    3-4: Related to medicine, the content involves medical fields.\\n    1-2: Very weak medical relevance.\\n4. Mention specific details: Whether specific case details in the text are mentioned.\\nPlease only output the results without any explanation and strictly follow the format below for output:\\n{\\n  \"quality\": 1-10,\\n  \"difficulty\": 1-10,\\n  \"relevance_to_medicine\": 1-6,\\n  \"mention_specific_details\": \"True\"/\"False\"\\n}\\nHere is the query:\\nHow do the results of gene transduction using GL-67:pDNA complexes in the nasal epithelium of cystic fibrosis transgenic mice compare to those observed in the lungs, and what implications do these differences have for the development of gene therapy strategies for cystic fibrosis?\\nOutput: \\n'})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question1_set[0], question2_set[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "text2prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages_template = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"{prompt}\"}\n",
    "]\n",
    "\n",
    "def create_messages(prompt):\n",
    "    return [\n",
    "        msg.copy() if msg[\"role\"] == \"system\" \n",
    "        else {\"role\": \"user\", \"content\": prompt}\n",
    "        for msg in messages_template\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_prompt(example):\n",
    "    message = create_messages(example[\"text\"])\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        message,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    example[\"prompt\"] = text\n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 209971/209971 [00:17<00:00, 11837.99 examples/s]\n",
      "Map: 100%|██████████| 209971/209971 [00:17<00:00, 11903.07 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(Dataset({\n",
       "     features: ['prompt'],\n",
       "     num_rows: 209971\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['prompt'],\n",
       "     num_rows: 209971\n",
       " }))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question1_set = question1_set.map(create_prompt, remove_columns=[\"text\"])\n",
    "question2_set = question2_set.map(create_prompt, remove_columns=[\"text\"])\n",
    "question1_set, question2_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'prompt': '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n<|im_start|>user\\nYou need to evaluate the given query based on the following criteria and output the results in JSON format. The output should include three parts: quality, difficulty, and whether additional necessary information is required to answer the query. Please follow the scoring standards below:\\n1. Quality (Score 1-10): Assess the clarity and accuracy of the query. If the query is a simple statement without any question or instruction, score it 1-2.\\n    9-10: Very clear, accurate expression, no ambiguity.\\n    7-8: Clear, accurate expression, but may have minimal ambiguity.\\n    5-6: Fairly clear, generally accurate expression, but some ambiguity exists.\\n    3-4: Not very clear, somewhat vague expression, with obvious ambiguity.\\n    1-2: Unclear, very vague expression, difficult to understand or a simple statement.\\n2. Difficulty (Score 1-10): Assess the difficulty of understanding and answering the query.\\n    9-10: Very difficult, requires specialized knowledge and complex analysis to answer.\\n    7-8: Quite difficult, requires some specialized knowledge and analysis.\\n    5-6: Moderate difficulty, requires general knowledge and analysis.\\n    3-4: Fairly simple, can be answered with basic knowledge.\\n    1-2: Very simple, no special knowledge required to answer.\\n3. Relevance to medicine (1-6): Assess the medical relevance of the query.\\n    5-6: Completely related to medicine, with many medical terms appearing.\\n    3-4: Related to medicine, the content involves medical fields.\\n    1-2: Very weak medical relevance.\\n4. Mention specific details: Whether specific case details in the text are mentioned.\\nPlease only output the results without any explanation and strictly follow the format below for output:\\n{\\n  \"quality\": 1-10,\\n  \"difficulty\": 1-10,\\n  \"relevance_to_medicine\": 1-6,\\n  \"mention_specific_details\": \"True\"/\"False\"\\n}\\nHere is the query:\\nWhat factors contribute to the low efficiency of cationic lipid-mediated gene transduction in polarized and differentiated airway epithelial cells, and how might these barriers be overcome to improve gene therapy for airway diseases such as cystic fibrosis?\\nOutput: \\n<|im_end|>\\n<|im_start|>assistant\\n'},\n",
       " {'prompt': '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n<|im_start|>user\\nYou need to evaluate the given query based on the following criteria and output the results in JSON format. The output should include three parts: quality, difficulty, and whether additional necessary information is required to answer the query. Please follow the scoring standards below:\\n1. Quality (Score 1-10): Assess the clarity and accuracy of the query. If the query is a simple statement without any question or instruction, score it 1-2.\\n    9-10: Very clear, accurate expression, no ambiguity.\\n    7-8: Clear, accurate expression, but may have minimal ambiguity.\\n    5-6: Fairly clear, generally accurate expression, but some ambiguity exists.\\n    3-4: Not very clear, somewhat vague expression, with obvious ambiguity.\\n    1-2: Unclear, very vague expression, difficult to understand or a simple statement.\\n2. Difficulty (Score 1-10): Assess the difficulty of understanding and answering the query.\\n    9-10: Very difficult, requires specialized knowledge and complex analysis to answer.\\n    7-8: Quite difficult, requires some specialized knowledge and analysis.\\n    5-6: Moderate difficulty, requires general knowledge and analysis.\\n    3-4: Fairly simple, can be answered with basic knowledge.\\n    1-2: Very simple, no special knowledge required to answer.\\n3. Relevance to medicine (1-6): Assess the medical relevance of the query.\\n    5-6: Completely related to medicine, with many medical terms appearing.\\n    3-4: Related to medicine, the content involves medical fields.\\n    1-2: Very weak medical relevance.\\n4. Mention specific details: Whether specific case details in the text are mentioned.\\nPlease only output the results without any explanation and strictly follow the format below for output:\\n{\\n  \"quality\": 1-10,\\n  \"difficulty\": 1-10,\\n  \"relevance_to_medicine\": 1-6,\\n  \"mention_specific_details\": \"True\"/\"False\"\\n}\\nHere is the query:\\nHow do the results of gene transduction using GL-67:pDNA complexes in the nasal epithelium of cystic fibrosis transgenic mice compare to those observed in the lungs, and what implications do these differences have for the development of gene therapy strategies for cystic fibrosis?\\nOutput: \\n<|im_end|>\\n<|im_start|>assistant\\n'})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question1_set[0], question2_set[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 209971/209971 [00:00<00:00, 386678.45 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 209971/209971 [00:00<00:00, 389323.73 examples/s]\n"
     ]
    }
   ],
   "source": [
    "question1_set.save_to_disk(\"data/scoring/ins1_prompt\")\n",
    "question2_set.save_to_disk(\"data/scoring/ins2_prompt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vllm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
